#!/usr/bin/env node

/**
 * Langfuse Prompt Management Demo
 * 
 * This demo showcases how product managers and content writers can update
 * prompts without requiring code changes or rebuilds. The application
 * dynamically fetches prompts from Langfuse at runtime.
 */

import { Langfuse } from 'langfuse';
import OpenAI from 'openai';
import readlineSync from 'readline-sync';
import dotenv from 'dotenv';

// Load environment variables
dotenv.config();

// Initialize clients
const langfuse = new Langfuse({
  secretKey: process.env.LANGFUSE_SECRET_KEY,
  publicKey: process.env.LANGFUSE_PUBLIC_KEY,
  baseUrl: process.env.LANGFUSE_HOST || 'http://localhost:3000',
});

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || 'sk-fake-key-for-demo'
});

// Demo scenarios
const DEMO_SCENARIOS = {
  'email_writer': {
    name: 'Email Writer Assistant',
    description: 'Generate professional emails based on user input',
    promptName: 'email-writer-v1',
    variables: ['recipient_name', 'email_purpose', 'tone']
  },
  'product_description': {
    name: 'Product Description Generator', 
    description: 'Create compelling product descriptions for e-commerce',
    promptName: 'product-description-v1',
    variables: ['product_name', 'key_features', 'target_audience']
  },
  'code_reviewer': {
    name: 'Code Review Assistant',
    description: 'Provide constructive feedback on code snippets',
    promptName: 'code-reviewer-v1', 
    variables: ['programming_language', 'code_snippet', 'review_focus']
  }
};

class PromptDemo {
  constructor() {
    this.currentScenario = null;
  }

  async fetchPromptFromLangfuse(promptName, label = 'production') {
    try {
      console.log(`\nðŸ”„ Fetching prompt "${promptName}" (label: ${label}) from Langfuse...`);
      
      const prompt = await langfuse.getPrompt(promptName, undefined, { label });
      
      if (!prompt) {
        throw new Error(`Prompt "${promptName}" not found`);
      }
      
      console.log(`âœ… Successfully retrieved prompt: ${prompt.name} v${prompt.version}`);
      console.log(`ðŸ“ Labels: ${prompt.labels.join(', ')}`);
      
      return prompt;
    } catch (error) {
      console.error(`âŒ Error fetching prompt: ${error.message}`);
      return null;
    }
  }

  compilePrompt(prompt, variables) {
    let compiledPrompt = prompt.prompt;
    
    // Replace variables in the format {{variable_name}}
    Object.entries(variables).forEach(([key, value]) => {
      const regex = new RegExp(`{{\\s*${key}\\s*}}`, 'g');
      compiledPrompt = compiledPrompt.replace(regex, value);
    });
    
    return compiledPrompt;
  }

  async simulateLLMCall(compiledPrompt, useRealLLM = false) {
    if (!useRealLLM || !process.env.OPENAI_API_KEY || process.env.OPENAI_API_KEY === 'sk-fake-key-for-demo') {
      // Simulate a response for demo purposes
      console.log(`\nðŸ¤– [SIMULATED] LLM Response:`);
      console.log(`"This is a simulated response to demonstrate prompt management.`);
      console.log(`In a real scenario, this would be generated by your LLM based on the prompt."`);
      return "Simulated LLM response based on the compiled prompt.";
    }
    
    try {
      const response = await openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: compiledPrompt }],
        max_tokens: 150
      });
      
      const content = response.choices[0]?.message?.content || 'No response generated';
      console.log(`\nðŸ¤– [REAL] LLM Response:`);
      console.log(`"${content}"`);
      return content;
    } catch (error) {
      console.error(`âŒ Error calling OpenAI: ${error.message}`);
      return this.simulateLLMCall(compiledPrompt, false);
    }
  }

  async traceToLangfuse(scenario, prompt, variables, compiledPrompt, llmResponse) {
    try {
      const trace = langfuse.trace({
        name: `prompt-demo-${scenario.promptName}`,
        tags: ['demo', 'prompt-management'],
        metadata: {
          scenario: scenario.name,
          promptVersion: prompt.version,
          promptLabels: prompt.labels
        }
      });

      const generation = trace.generation({
        name: 'prompt-execution',
        model: 'demo-model',
        input: variables,
        output: llmResponse,
        prompt: {
          name: prompt.name,
          version: prompt.version
        }
      });

      await langfuse.flushAsync();
      console.log(`\nðŸ“Š Trace logged to Langfuse: ${trace.id}`);
    } catch (error) {
      console.error(`âŒ Error logging to Langfuse: ${error.message}`);
    }
  }

  collectVariableValues(scenario) {
    console.log(`\nðŸ“ Please provide values for the following variables:`);
    const variables = {};
    
    scenario.variables.forEach(variable => {
      const value = readlineSync.question(`${variable}: `);
      variables[variable] = value || `[sample_${variable}]`;
    });
    
    return variables;
  }

  async runScenario(scenarioKey) {
    const scenario = DEMO_SCENARIOS[scenarioKey];
    if (!scenario) {
      console.log(`âŒ Scenario "${scenarioKey}" not found`);
      return;
    }

    console.log(`\nðŸš€ Running Demo: ${scenario.name}`);
    console.log(`ðŸ“„ Description: ${scenario.description}`);
    console.log(`ðŸŽ¯ Prompt Name: ${scenario.promptName}`);

    // Step 1: Fetch prompt from Langfuse
    const label = readlineSync.question(`\nWhich prompt version would you like to use? (Enter label, default: production): `) || 'production';
    const prompt = await this.fetchPromptFromLangfuse(scenario.promptName, label);
    
    if (!prompt) {
      console.log(`âŒ Could not fetch prompt. Make sure "${scenario.promptName}" exists in Langfuse.`);
      return;
    }

    // Step 2: Show the prompt template
    console.log(`\nðŸ“‹ Prompt Template:`);
    console.log(`"${prompt.prompt}"`);

    // Step 3: Collect variable values
    const variables = this.collectVariableValues(scenario);
    
    // Step 4: Compile the prompt
    const compiledPrompt = this.compilePrompt(prompt, variables);
    console.log(`\nâœ¨ Compiled Prompt:`);
    console.log(`"${compiledPrompt}"`);

    // Step 5: Simulate LLM call
    const useRealLLM = readlineSync.keyInYNStrict('\nWould you like to make a real LLM call? (requires OpenAI API key)');
    const llmResponse = await this.simulateLLMCall(compiledPrompt, useRealLLM);

    // Step 6: Log to Langfuse
    await this.traceToLangfuse(scenario, prompt, variables, compiledPrompt, llmResponse);

    console.log(`\nâœ… Demo scenario "${scenario.name}" completed!`);
  }

  showMainMenu() {
    console.log(`\nðŸŽ­ Langfuse Prompt Management Demo`);
    console.log(`=================================`);
    console.log(`\nAvailable Scenarios:`);
    
    Object.entries(DEMO_SCENARIOS).forEach(([key, scenario], index) => {
      console.log(`${index + 1}. ${scenario.name} - ${scenario.description}`);
    });
    
    console.log(`\nOther Options:`);
    console.log(`4. Show Prompt Management Benefits`);
    console.log(`5. Setup Instructions`);
    console.log(`6. Exit`);
  }

  showBenefits() {
    console.log(`\nðŸŒŸ Langfuse Prompt Management Benefits`);
    console.log(`=====================================`);
    console.log(`\nâœ… Decoupled Prompts from Code:`);
    console.log(`   â€¢ PMs and Writers can update prompts without touching code`);
    console.log(`   â€¢ No need for developers to make prompt changes`);
    console.log(`   â€¢ No application rebuilds or deployments required`);
    
    console.log(`\nðŸ”„ Version Control & Rollback:`);
    console.log(`   â€¢ Track all prompt changes with versions`);
    console.log(`   â€¢ Easy rollback to previous versions if needed`);
    console.log(`   â€¢ Compare different versions side-by-side`);
    
    console.log(`\nðŸ·ï¸ Label-based Deployment:`);
    console.log(`   â€¢ Use labels like 'production', 'staging', 'experimental'`);
    console.log(`   â€¢ Switch between prompt versions without code changes`);
    console.log(`   â€¢ A/B testing different prompts in production`);
    
    console.log(`\nðŸ“Š Observability & Analytics:`);
    console.log(`   â€¢ Track prompt performance and usage`);
    console.log(`   â€¢ Monitor which prompts work best`);
    console.log(`   â€¢ Integration with your LLM traces and metrics`);
    
    console.log(`\nðŸ‘¥ Team Collaboration:`);
    console.log(`   â€¢ Non-technical team members can manage prompts`);
    console.log(`   â€¢ Review and approval workflows`);
    console.log(`   â€¢ Audit trail of all changes`);
  }

  showSetupInstructions() {
    console.log(`\nâš™ï¸ Demo Setup Instructions`);
    console.log(`=========================`);
    console.log(`\n1. Start Langfuse locally:`);
    console.log(`   cd langfuse`);
    console.log(`   pnpm run dev:web`);
    
    console.log(`\n2. Access Langfuse at http://localhost:3000`);
    console.log(`   Login: demo@langfuse.com`);
    console.log(`   Password: password`);
    
    console.log(`\n3. Create sample prompts in the UI:`);
    console.log(`   â€¢ Go to Prompts section`);
    console.log(`   â€¢ Create prompts named: email-writer-v1, product-description-v1, code-reviewer-v1`);
    console.log(`   â€¢ Use variables like {{recipient_name}}, {{product_name}}, etc.`);
    console.log(`   â€¢ Label them as 'production'`);
    
    console.log(`\n4. Configure environment variables:`);
    console.log(`   â€¢ Copy .env.example to .env`);
    console.log(`   â€¢ Add your Langfuse API keys from the settings page`);
    console.log(`   â€¢ Optionally add OpenAI API key for real LLM calls`);
    
    console.log(`\n5. Install dependencies and run:`);
    console.log(`   npm install`);
    console.log(`   npm start`);
  }

  async run() {
    console.log(`\nðŸŽ‰ Welcome to the Langfuse Prompt Management Demo!`);
    
    while (true) {
      this.showMainMenu();
      
      const choice = readlineSync.question('\nSelect an option (1-6): ');
      
      switch (choice) {
        case '1':
          await this.runScenario('email_writer');
          break;
        case '2':
          await this.runScenario('product_description');
          break;
        case '3':
          await this.runScenario('code_reviewer');
          break;
        case '4':
          this.showBenefits();
          break;
        case '5':
          this.showSetupInstructions();
          break;
        case '6':
          console.log('\nðŸ‘‹ Thank you for trying the Langfuse Prompt Management Demo!');
          await langfuse.shutdownAsync();
          process.exit(0);
          break;
        default:
          console.log('\nâŒ Invalid choice. Please select 1-6.');
      }
      
      if (choice !== '6') {
        readlineSync.question('\nPress Enter to continue...');
      }
    }
  }
}

// Handle graceful shutdown
process.on('SIGINT', async () => {
  console.log('\n\nðŸ‘‹ Shutting down gracefully...');
  await langfuse.shutdownAsync();
  process.exit(0);
});

// Start the demo
const demo = new PromptDemo();
demo.run().catch(console.error);